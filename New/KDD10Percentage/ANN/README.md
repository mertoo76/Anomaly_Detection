## Model 1
![alt text](model1.png)

activation function between hidden layer = relu

activation function output layer = sigmoid

4/60/60/60/1

![alt text](Model1Result.png)

## Model 2
![alt text](model2.png)

activation function between hidden layer = relu

activation function output layer = sigmoid

Without Dropout 4/16/16/1

![alt text](Model2Result.png)

## Model 3
![alt text](model3.png)

activation function between hidden layer = relu

activation function output layer = softmax

4/60/60/60/2

![alt text](Model3Result.png)

## Model 4
![alt text](model4.png)

activation function between hidden layer = relu

activation function output layer = sigmoid

4/30/30/1

![alt text](Model4Result.png)

## Model 5
![alt text](model5.png)

activation function between hidden layer = relu

activation function output layer = sigmoid

4/60/60/60/1

![alt text](Model5Result.png)

## Model 6
![alt text](model6.png)

activation function between hidden layer = relu

activation function output layer = softmax

4/16/16/2

![alt text](Model6Result.png)

## Model 6
![alt text](model7.png)

activation function between hidden layer = relu

activation function output layer = softmax

4/16/16/1

![alt text](Model7Result.png)